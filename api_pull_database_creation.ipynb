{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (ArrayType, LongType, StringType, StructField, StructType, DoubleType, MapType)\n",
    "import json\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.safegraph.com/v2/graphql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate API connection\n",
    "transport = RequestsHTTPTransport(\n",
    "    url=url,\n",
    "    verify=True,\n",
    "    retries=3,\n",
    "    headers={'Content-Type': 'application/json', 'apikey': sfkey})\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sg = \"\"\"query {\n",
    "  search(filter: { \n",
    "     --FILTERS--\n",
    "    address: {\n",
    "      region: \"--STATENAME--\"\n",
    "    }\n",
    "  }){\n",
    "    places {\n",
    "      results(first: 500 after: \"--ENDCURSER--\") {\n",
    "        pageInfo { hasNextPage, endCursor}\n",
    "        edges {\n",
    "          node {\n",
    "            monthly_patterns (start_date: \"--DATESTART--\" end_date: \"--DATEEND--\") {\n",
    "              placekey\n",
    "              parent_placekey\n",
    "              location_name\n",
    "              street_address\n",
    "              city\n",
    "              region\n",
    "              postal_code\n",
    "              iso_country_code\n",
    "              date_range_start\n",
    "              date_range_end\n",
    "              raw_visit_counts\n",
    "              raw_visitor_counts\n",
    "              visits_by_day\n",
    "              device_type\n",
    "              poi_cbg\n",
    "              visitor_home_cbgs\n",
    "              visitor_home_aggregation\n",
    "              visitor_daytime_cbgs\n",
    "              visitor_country_of_origin\n",
    "              distance_from_home\n",
    "              median_dwell\n",
    "              bucketed_dwell_times\n",
    "              related_same_day_brand\n",
    "              related_same_month_brand\n",
    "              normalized_visits_by_total_visits\n",
    "              normalized_visits_by_state_scaling\n",
    "              normalized_visits_by_total_visitors\n",
    "              normalized_visits_by_region_naics_visits\n",
    "              normalized_visits_by_region_naics_visitors\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('/tmp/api_challenge')\n",
    "dbutils.fs.mkdirs(\"/FileStore/api_challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPaging = ''\n",
    "while True: \n",
    "    query_sg_text = query_sg\\\n",
    "      .replace(\"--STATENAME--\", \"RI\")\\\n",
    "      .replace(\"--FILTERS--\", '')\\\n",
    "      .replace(\"--DATESTART--\", \"2022-01-01\")\\\n",
    "      .replace(\"--DATEEND--\", \"2022-04-01\")\\\n",
    "      .replace(\"--ENDCURSER--\", nextPaging)\n",
    "    sgIter = client.execute(gql(query_sg_text))\n",
    "    pageInformation = sgIter['search']['places']['results']['pageInfo']\n",
    "    nextPaging = pageInformation['endCursor']\n",
    "    edgesIter = sgIter['search']['places']['results']['edges']\n",
    "    sgIter = [dat.pop('node') for dat in edgesIter]\n",
    "    sgIter = [dat.pop('monthly_patterns') for dat in sgIter]\n",
    "    with jsonlines.open(\"/tmp/api_challenge/api_2.jl\", 'a') as writer:\n",
    "        writer.write_all(sgIter)\n",
    "        writer.close()\n",
    "    if nextPaging is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"/tmp/api_challenge\"))\n",
    "dbutils.fs.ls(\"File:/tmp/api_challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"File:/tmp/api_challenge/api_2.jl\", \"dbfs:/FileStore/api_challenge/api_2.jl\")\n",
    "dbutils.fs.cp(\"File:/tmp/api_challenge/\", \"dbfs:/FileStore/api_challenge/\", recurse = True)\n",
    "dbutils.fs.ls(\"/FileStore/api_challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsg = spark.read.json(\"dbfs:/FileStore/api_challenge/api_2.jl\", schema = schema)\n",
    "dfsg = spark.read.json(\"dbfs:/FileStore/api_challenge\", schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE api1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsg = dfsg.na.drop(subset=[\"placekey\"])\n",
    "dfsg = dfsg.persist()\n",
    "print(\"Count with all json files: \" + str(dfsg.count()))\n",
    "\n",
    "dfsg = dfsg.dropDuplicates(['placekey', 'date_range_start'])\n",
    "dfsg.persist()\n",
    "print(\"Count after duplicates: \" + str(dfsg.count()))\n",
    "\n",
    "dfsg.unpersist()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsg.repartition(10).write.format(\"delta\").saveAsTable(\"api1.test_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM api1.test_table\")\n",
    "df = spark.table(\"api1.test_table\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
